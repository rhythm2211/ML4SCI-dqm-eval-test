{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11273541,"sourceType":"datasetVersion","datasetId":7047518}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML4SCI GSoC 2025 – ML4DQM Evaluation Test 1: ViT Classification Report\n\n## Objective\n\nDevelop a machine learning model using a Vision Transformer (ViT) architecture to classify images (HCAL DigiOccupancy maps) based on their originating synthetic dataset (`Run357479` or `Run355456`).\n\n---\n\n## 1. Model Architecture & Hyperparameters\n\n- **Model Type**: Vision Transformer (ViT) using **TensorFlow 2.17.1** + **Keras**\n- **Input Shape**: `(64, 72, 1)` → `(ieta, iphi, channels)`\n- **Patch Size**: `(8, 9)` → 64 non-overlapping patches\n- **Projection Dimension**: `128` (per patch)\n- **Transformer Layers**: `4`\n- **Attention Heads**: `4`\n- **MLP Dimension (per transformer block)**: `256`\n- **Classifier Head**:\n  - Global Average Pooling\n  - Dropout (0.5)\n  - Dense (64 units, ReLU)\n  - Dropout (0.3)\n  - Dense (1 unit, Sigmoid)\n\n- **Total Parameters**: ~556,033 (all trainable)\n\n---\n\n##  2. Data Preprocessing\n\n- **Datasets**:\n  - `/kaggle/input/gsoc-test-data/Run357479_Dataset_iodic.npy`\n  - `/kaggle/input/gsoc-test-data/Run355456_Dataset_jqkne.npy`\n  - Shape: `(10000, 64, 72)` each\n\n- **Steps**:\n  - **Concatenation**: Combined to shape `(20000, 64, 72)`\n  - **Labels**: `0` → Run357479, `1` → Run355456\n  - **Reshape**: Add channel dim → `(20000, 64, 72, 1)`\n  - **Split**: \n    - Train (60%): `12000` samples  \n    - Validation (20%): `4000` samples  \n    - Test (20%): `4000` samples  \n    - **Stratified split**\n  - **Normalization**:\n    - Min-Max scaling to `[0, 1]` using only training set min (0.0) and max (~1564.9)\n  - **Data Types**:\n    - Images → `float32`\n    - Labels → `int32`\n\n---\n\n##  3. Training\n\n- **Framework**: TensorFlow 2.17.1 + Keras\n- **Hardware**: 2× NVIDIA T4 GPUs via `tf.distribute.MirroredStrategy`\n\n- **Data Pipeline**:\n  - `tf.data.Dataset` with:\n    - Caching\n    - Shuffling (buffer size = 12000)\n    - Batching (batch size = 64)\n    - Prefetching\n\n- **Optimizer**: AdamW (LR = `3e-4`, weight decay = `1e-5`)\n- **Loss Function**: Binary Crossentropy\n- **Metrics**:\n  - Binary Accuracy\n  - ROC AUC\n  - PRC AUC\n\n- **Callbacks**:\n  - `ModelCheckpoint`: Save best model (`val_auc`)\n  - `EarlyStopping`: Patience = 10, restore best weights\n\n- **Training Duration**:\n  - Stopped early at Epoch 13\n  - Best model: Epoch 3 (`val_auc = 1.0`)\n  - Total time: ~153 seconds\n\n---\n\n##  4. Evaluation Results\n\n- **On Test Set**:\n  - **Loss**: `0.0000`\n  - **Accuracy**: `1.0000`\n  - **ROC AUC**: `1.0000`\n  - **PRC AUC**: `1.0000`\n\n- **ROC Curve**:\n  ![ROC Curve](roc_curve_test_set.png)\n\n  > _Perfect ROC curve with AUC = 1.0_\n\n---\n\n##  5. Conclusion\n\nThe Vision Transformer achieved **perfect classification** performance:\n- Accuracy = `1.0`\n- ROC AUC = `1.0`\n\n### Highlights:\n- Synthetic datasets are clearly distinguishable via spatial occupancy patterns.\n- ViT effectively captured those patterns.\n- Training completed in a short time with high generalization performance.\n\n\n\nBy,\n- Rhythm Suthar\n- Email : rhythmsuthar123@gmail.com\n\n---\n","metadata":{}},{"cell_type":"code","source":"!pip install numpy tensorflow scikit-learn matplotlib wget requests # Added wget/requests for download\n# Optional but recommended: A ViT implementation helper library\n!pip install vit-keras # Example for Keras, or use Hugging Face Transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\n\n# Path for Run 357479 data\nfile_path_run357479 = '/kaggle/input/gsoc-test-data/Run357479_Dataset_iodic.npy'\n# Path for Run 355456 data\nfile_path_run355456 = '/kaggle/input/gsoc-test-data/Run355456_Dataset_jqkne.npy'\n# ---\n\nprint(f\"Inspecting Run 357479 file: {file_path_run357479}\")\ndata_run357479 = None # Initialize\ntry:\n    # Check existence first\n    if os.path.exists(file_path_run357479):\n        data_run357479 = np.load(file_path_run357479)\n        print(f\"  - Successfully loaded.\")\n        print(f\"  - Shape: {data_run357479.shape}\")\n        print(f\"  - Data Type (dtype): {data_run357479.dtype}\")\n        \n        # Calculate stats only if data is loaded and non-empty\n        if data_run357479.size > 0:\n            print(f\"  - Minimum value: {np.min(data_run357479)}\")\n            print(f\"  - Maximum value: {np.max(data_run357479)}\")\n            print(f\"  - Mean value: {np.mean(data_run357479):.4f}\")\n            zero_percentage = (np.count_nonzero(data_run357479==0) / data_run357479.size) * 100\n            print(f\"  - Percentage of zero entries: {zero_percentage:.2f}%\")\n        else:\n            print(\"  - Array is empty, skipping stats.\")\n            \n        print(\"  - Sample slice [0, :3, :3]:\")\n        # Check dimensions and size before slicing\n        if data_run357479.ndim >= 3 and data_run357479.shape[0] > 0 and data_run357479.shape[1] > 2 and data_run357479.shape[2] > 2:\n             print(data_run357479[0, :3, :3])\n        else:\n            print(\"  - Not enough data/dimensions for sample slice.\")\n            \n    else:\n        print(f\"  - Error: File not found at path: {file_path_run357479}\")\n        # Try listing parent dir for debugging\n        parent_dir = os.path.dirname(file_path_run357479)\n        print(f\"  - Checking parent directory: {parent_dir}\")\n        try:\n            if os.path.exists(parent_dir): print(f\"    - Contents: {os.listdir(parent_dir)}\")\n            else: print(f\"    - Parent directory does not exist.\")\n        except Exception as list_e: print(f\"    - Error listing parent directory: {list_e}\")\n        \nexcept Exception as e:\n    print(f\"  - Error loading or inspecting file: {e}\")\n\nprint(\"-\" * 30)\n\nprint(f\"Inspecting Run 355456 file: {file_path_run355456}\")\ndata_run355456 = None # Initialize\ntry:\n    # Check existence first\n    if os.path.exists(file_path_run355456):\n        data_run355456 = np.load(file_path_run355456)\n        print(f\"  - Successfully loaded.\")\n        print(f\"  - Shape: {data_run355456.shape}\")\n        print(f\"  - Data Type (dtype): {data_run355456.dtype}\")\n        \n        # Calculate stats only if data is loaded and non-empty\n        if data_run355456.size > 0:\n            print(f\"  - Minimum value: {np.min(data_run355456)}\")\n            print(f\"  - Maximum value: {np.max(data_run355456)}\")\n            print(f\"  - Mean value: {np.mean(data_run355456):.4f}\")\n            zero_percentage = (np.count_nonzero(data_run355456==0) / data_run355456.size) * 100\n            print(f\"  - Percentage of zero entries: {zero_percentage:.2f}%\")\n        else:\n            print(\"  - Array is empty, skipping stats.\")\n            \n        print(\"  - Sample slice [0, :3, :3]:\")\n        # Check dimensions and size before slicing\n        if data_run355456.ndim >= 3 and data_run355456.shape[0] > 0 and data_run355456.shape[1] > 2 and data_run355456.shape[2] > 2:\n             print(data_run355456[0, :3, :3])\n        else:\n            print(\"  - Not enough data/dimensions for sample slice.\")\n            \n    else:\n        print(f\"  - Error: File not found at path: {file_path_run355456}\")\n        # Try listing parent dir for debugging\n        parent_dir = os.path.dirname(file_path_run355456)\n        print(f\"  - Checking parent directory: {parent_dir}\")\n        try:\n            if os.path.exists(parent_dir): print(f\"    - Contents: {os.listdir(parent_dir)}\")\n            else: print(f\"    - Parent directory does not exist.\")\n        except Exception as list_e: print(f\"    - Error listing parent directory: {list_e}\")\n\nexcept Exception as e:\n    print(f\"  - Error loading or inspecting file: {e}\")\n\n# Comparison\ndata1_loaded = data_run357479 is not None\ndata2_loaded = data_run355456 is not None\n\nif data1_loaded and data2_loaded:\n     print(\"-\" * 30)\n     print(\"Comparison:\")\n     # Check shapes only if both arrays actually have shapes\n     if hasattr(data_run357479, 'shape') and hasattr(data_run355456, 'shape'):\n         if data_run357479.shape == data_run355456.shape:\n             print(f\"  - Shapes match: {data_run357479.shape}\")\n         else:\n             print(f\"  - Shapes DIFFER: {data_run357479.shape} vs {data_run355456.shape}\")\n     # Check dtypes only if both arrays actually have dtypes\n     if hasattr(data_run357479, 'dtype') and hasattr(data_run355456, 'dtype'):\n         if data_run357479.dtype == data_run355456.dtype:\n             print(f\"  - Data types match: {data_run357479.dtype}\")\n         else:\n             print(f\"  - Data types DIFFER: {data_run357479.dtype} vs {data_run355456.dtype}\")\nelif data1_loaded or data2_loaded:\n    print(\"-\" * 30)\n    print(\"Comparison skipped as only one file was loaded successfully.\")\nelse:\n    print(\"-\" * 30)\n    print(\"Comparison skipped as neither file loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:38:59.432818Z","iopub.execute_input":"2025-04-04T10:38:59.433254Z","iopub.status.idle":"2025-04-04T10:39:00.096480Z","shell.execute_reply.started":"2025-04-04T10:38:59.433224Z","shell.execute_reply":"2025-04-04T10:39:00.095675Z"}},"outputs":[{"name":"stdout","text":"Inspecting Run 357479 file: /kaggle/input/gsoc-test-data/Run357479_Dataset_iodic.npy\n  - Successfully loaded.\n  - Shape: (10000, 64, 72)\n  - Data Type (dtype): float64\n  - Minimum value: 0.0\n  - Maximum value: 1091.9733311864536\n  - Mean value: 181.0826\n  - Percentage of zero entries: 79.77%\n  - Sample slice [0, :3, :3]:\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n------------------------------\nInspecting Run 355456 file: /kaggle/input/gsoc-test-data/Run355456_Dataset_jqkne.npy\n  - Successfully loaded.\n  - Shape: (10000, 64, 72)\n  - Data Type (dtype): float64\n  - Minimum value: 0.0\n  - Maximum value: 1564.944737802157\n  - Mean value: 157.1423\n  - Percentage of zero entries: 79.77%\n  - Sample slice [0, :3, :3]:\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n------------------------------\nComparison:\n  - Shapes match: (10000, 64, 72)\n  - Data types match: float64\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\nfile_path_run357479 = '/kaggle/input/gsoc-test-data/Run357479_Dataset_iodic.npy'\nfile_path_run355456 = '/kaggle/input/gsoc-test-data/Run355456_Dataset_jqkne.npy'\n# ---\n\nprint(\"Starting EDA Step 1: Value Distributions (Non-Zero)\")\n\ntry:\n    # Load data\n    print(\"Loading data...\")\n    data_run357479 = np.load(file_path_run357479)\n    data_run355456 = np.load(file_path_run355456)\n    print(\"Data loaded.\")\n\n    # --- Analyze Run 357479 ---\n    print(\"\\nAnalyzing Run 357479...\")\n    # Flatten the array to get a 1D list of all values\n    flat_data_357479 = data_run357479.flatten()\n    # Select only non-zero values\n    nonzero_data_357479 = flat_data_357479[flat_data_357479 > 0]\n    \n    if nonzero_data_357479.size > 0:\n        print(f\"  Found {nonzero_data_357479.size} non-zero values (out of {flat_data_357479.size}).\")\n        # Plot histogram\n        plt.figure(figsize=(10, 6))\n        plt.hist(nonzero_data_357479, bins=100, log=True) # Use 100 bins, log scale for y-axis\n        plt.title('Distribution of Non-Zero DigiOccupancy Values (Run 357479)')\n        plt.xlabel('DigiOccupancy Value')\n        plt.ylabel('Frequency (Log Scale)')\n        plt.grid(True, axis='y', linestyle='--')\n        plt.savefig('histogram_run357479_nonzero.png')\n        print(\"  Saved histogram to histogram_run357479_nonzero.png\")\n        plt.close() # Close the plot to prevent display issues if running many plots\n    else:\n        print(\"  No non-zero values found in Run 357479.\")\n\n    # --- Analyze Run 355456 ---\n    print(\"\\nAnalyzing Run 355456...\")\n    # Flatten the array\n    flat_data_355456 = data_run355456.flatten()\n    # Select only non-zero values\n    nonzero_data_355456 = flat_data_355456[flat_data_355456 > 0]\n\n    if nonzero_data_355456.size > 0:\n        print(f\"  Found {nonzero_data_355456.size} non-zero values (out of {flat_data_355456.size}).\")\n        # Plot histogram\n        plt.figure(figsize=(10, 6))\n        plt.hist(nonzero_data_355456, bins=100, log=True) # Use 100 bins, log scale for y-axis\n        plt.title('Distribution of Non-Zero DigiOccupancy Values (Run 355456)')\n        plt.xlabel('DigiOccupancy Value')\n        plt.ylabel('Frequency (Log Scale)')\n        plt.grid(True, axis='y', linestyle='--')\n        plt.savefig('histogram_run355456_nonzero.png')\n        print(\"  Saved histogram to histogram_run355456_nonzero.png\")\n        plt.close() # Close the plot\n    else:\n        print(\"  No non-zero values found in Run 355456.\")\n\n    print(\"\\nEDA Step 1 finished.\")\n\nexcept FileNotFoundError:\n    print(\"Error: One or both input files not found. Please ensure paths are correct.\")\nexcept Exception as e:\n    print(f\"An error occurred during EDA Step 1: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:15:10.760472Z","iopub.execute_input":"2025-04-04T10:15:10.760796Z","iopub.status.idle":"2025-04-04T10:15:13.349133Z","shell.execute_reply.started":"2025-04-04T10:15:10.760774Z","shell.execute_reply":"2025-04-04T10:15:13.348034Z"}},"outputs":[{"name":"stdout","text":"Starting EDA Step 1: Value Distributions (Non-Zero)\nLoading data...\nData loaded.\n\nAnalyzing Run 357479...\n  Found 9320000 non-zero values (out of 46080000).\n  Saved histogram to histogram_run357479_nonzero.png\n\nAnalyzing Run 355456...\n  Found 9320000 non-zero values (out of 46080000).\n  Saved histogram to histogram_run355456_nonzero.png\n\nEDA Step 1 finished.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\nfile_path_run357479 = '/kaggle/input/gsoc-test-data/Run357479_Dataset_iodic.npy'\nfile_path_run355456 = '/kaggle/input/gsoc-test-data/Run355456_Dataset_jqkne.npy'\n# ---\n\nprint(\"Starting EDA Step 2: Average Spatial Patterns\")\n\ntry:\n    # Load data\n    print(\"Loading data...\")\n    data_run357479 = np.load(file_path_run357479)\n    data_run355456 = np.load(file_path_run355456)\n    print(\"Data loaded.\")\n\n    # --- Calculate Average Maps ---\n    print(\"Calculating average maps (mean across samples)...\")\n    # axis=0 averages over the 'LS' or 'sample' dimension\n    mean_map_357479 = np.mean(data_run357479, axis=0)\n    mean_map_355456 = np.mean(data_run355456, axis=0)\n    print(f\"  Shape of average maps: {mean_map_357479.shape}\") # Should be (64, 72)\n\n    # Determine a common color scale for comparison\n    vmin = min(np.min(mean_map_357479), np.min(mean_map_355456))\n    vmax = max(np.max(mean_map_357479), np.max(mean_map_355456))\n    print(f\"  Common color scale range: min={vmin:.2f}, max={vmax:.2f}\")\n\n    # --- Plot Average Map for Run 357479 ---\n    print(\"Plotting average map for Run 357479...\")\n    plt.figure(figsize=(10, 8))\n    im = plt.imshow(mean_map_357479, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax)\n    plt.colorbar(im, label='Average DigiOccupancy')\n    plt.title('Average DigiOccupancy Map (Run 357479)')\n    plt.xlabel('iPhi Index')\n    plt.ylabel('iEta Index')\n    plt.savefig('average_map_run357479.png')\n    print(\"  Saved average map to average_map_run357479.png\")\n    plt.close()\n\n    # --- Plot Average Map for Run 355456 ---\n    print(\"\\nPlotting average map for Run 355456...\")\n    plt.figure(figsize=(10, 8))\n    im = plt.imshow(mean_map_355456, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax) # Use same vmin/vmax\n    plt.colorbar(im, label='Average DigiOccupancy')\n    plt.title('Average DigiOccupancy Map (Run 355456)')\n    plt.xlabel('iPhi Index')\n    plt.ylabel('iEta Index')\n    plt.savefig('average_map_run355456.png')\n    print(\"  Saved average map to average_map_run355456.png\")\n    plt.close()\n\n    print(\"\\nEDA Step 2 finished.\")\n\nexcept FileNotFoundError:\n    print(\"Error: One or both input files not found. Please ensure paths are correct.\")\nexcept Exception as e:\n    print(f\"An error occurred during EDA Step 2: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:39:15.720185Z","iopub.execute_input":"2025-04-04T10:39:15.720540Z","iopub.status.idle":"2025-04-04T10:39:16.614007Z","shell.execute_reply.started":"2025-04-04T10:39:15.720518Z","shell.execute_reply":"2025-04-04T10:39:16.613181Z"}},"outputs":[{"name":"stdout","text":"Starting EDA Step 2: Average Spatial Patterns\nLoading data...\nData loaded.\nCalculating average maps (mean across samples)...\n  Shape of average maps: (64, 72)\n  Common color scale range: min=0.00, max=1455.40\nPlotting average map for Run 357479...\n  Saved average map to average_map_run357479.png\n\nPlotting average map for Run 355456...\n  Saved average map to average_map_run355456.png\n\nEDA Step 2 finished.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nimport os\n\n\nfile_path_run357479 = '/kaggle/input/gsoc-test-data/Run357479_Dataset_iodic.npy'\nfile_path_run355456 = '/kaggle/input/gsoc-test-data/Run355456_Dataset_jqkne.npy'\n# ---\n\nprint(\"Starting EDA Step 2b: Statistical Analysis of Average Spatial Patterns\")\n\ntry:\n    # Load data\n    print(\"Loading data...\")\n    data_run357479 = np.load(file_path_run357479)\n    data_run355456 = np.load(file_path_run355456)\n    print(\"Data loaded.\")\n\n    # --- Calculate Average Maps ---\n    print(\"\\nCalculating average maps...\")\n    mean_map_357479 = np.mean(data_run357479, axis=0)\n    mean_map_355456 = np.mean(data_run355456, axis=0)\n    print(\"Average maps calculated.\")\n\n    # --- Analyze Average Map Stats ---\n    print(\"\\n--- Statistics for Average Map: Run 357479 ---\")\n    print(f\"  Min: {np.min(mean_map_357479):.4f}\")\n    print(f\"  Max: {np.max(mean_map_357479):.4f}\")\n    print(f\"  Mean: {np.mean(mean_map_357479):.4f}\")\n    print(f\"  Std Dev: {np.std(mean_map_357479):.4f}\")\n    print(f\"  Median (50th %): {np.median(mean_map_357479):.4f}\")\n    print(f\"  75th Percentile: {np.percentile(mean_map_357479, 75):.4f}\")\n\n    print(\"\\n--- Statistics for Average Map: Run 355456 ---\")\n    print(f\"  Min: {np.min(mean_map_355456):.4f}\")\n    print(f\"  Max: {np.max(mean_map_355456):.4f}\")\n    print(f\"  Mean: {np.mean(mean_map_355456):.4f}\") # Should match overall mean from inspection\n    print(f\"  Std Dev: {np.std(mean_map_355456):.4f}\")\n    print(f\"  Median (50th %): {np.median(mean_map_355456):.4f}\")\n    print(f\"  75th Percentile: {np.percentile(mean_map_355456, 75):.4f}\")\n\n    # --- Find Top N Active Cells ---\n    N_TOP = 5\n    print(f\"\\n--- Top {N_TOP} Most Active Cells (Highest Average Occupancy) ---\")\n    \n    # For Run 357479\n    indices_flat_357479 = np.argsort(mean_map_357479.flatten())[-N_TOP:][::-1] # Get indices of top N, reversed for descending order\n    coords_357479 = [np.unravel_index(i, mean_map_357479.shape) for i in indices_flat_357479]\n    print(\"  Run 357479:\")\n    for i, coord in enumerate(coords_357479):\n        print(f\"    #{i+1}: iEta={coord[0]}, iPhi={coord[1]} -> Avg Value={mean_map_357479[coord]:.2f}\")\n\n    # For Run 355456\n    indices_flat_355456 = np.argsort(mean_map_355456.flatten())[-N_TOP:][::-1]\n    coords_355456 = [np.unravel_index(i, mean_map_355456.shape) for i in indices_flat_355456]\n    print(\"  Run 355456:\")\n    for i, coord in enumerate(coords_355456):\n        print(f\"    #{i+1}: iEta={coord[0]}, iPhi={coord[1]} -> Avg Value={mean_map_355456[coord]:.2f}\")\n\n\n    # --- Analyze Difference Map ---\n    print(\"\\n--- Analysis of Difference Map (Run357479 - Run355456) ---\")\n    diff_map = mean_map_357479 - mean_map_355456\n    \n    print(\"  Difference Map Statistics:\")\n    print(f\"    Min Diff: {np.min(diff_map):.4f}\")\n    print(f\"    Max Diff: {np.max(diff_map):.4f}\")\n    print(f\"    Mean Diff: {np.mean(diff_map):.4f}\") # Run357479_mean - Run355456_mean\n    print(f\"    Std Dev Diff: {np.std(diff_map):.4f}\")\n    \n    # Overall difference magnitude\n    mean_abs_diff = np.mean(np.abs(diff_map))\n    print(f\"    Mean Absolute Difference: {mean_abs_diff:.4f}\")\n\n    # Find Top N Differences\n    print(f\"\\n  Top {N_TOP} Largest Positive Differences (Run357479 > Run355456):\")\n    indices_flat_diff_pos = np.argsort(diff_map.flatten())[-N_TOP:][::-1]\n    coords_diff_pos = [np.unravel_index(i, diff_map.shape) for i in indices_flat_diff_pos]\n    for i, coord in enumerate(coords_diff_pos):\n        print(f\"    #{i+1}: iEta={coord[0]}, iPhi={coord[1]} -> Diff={diff_map[coord]:.2f} (Vals: {mean_map_357479[coord]:.2f} vs {mean_map_355456[coord]:.2f})\")\n\n    print(f\"\\n  Top {N_TOP} Largest Negative Differences (Run357479 < Run355456):\")\n    indices_flat_diff_neg = np.argsort(diff_map.flatten())[:N_TOP] # Get indices of smallest N\n    coords_diff_neg = [np.unravel_index(i, diff_map.shape) for i in indices_flat_diff_neg]\n    for i, coord in enumerate(coords_diff_neg):\n        print(f\"    #{i+1}: iEta={coord[0]}, iPhi={coord[1]} -> Diff={diff_map[coord]:.2f} (Vals: {mean_map_357479[coord]:.2f} vs {mean_map_355456[coord]:.2f})\")\n\n\n    print(\"\\nEDA Step 2b finished.\")\n\nexcept FileNotFoundError:\n    print(\"Error: One or both input files not found. Please ensure paths are correct.\")\nexcept Exception as e:\n    print(f\"An error occurred during EDA Step 2b: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:39:25.439316Z","iopub.execute_input":"2025-04-04T10:39:25.439668Z","iopub.status.idle":"2025-04-04T10:39:25.785881Z","shell.execute_reply.started":"2025-04-04T10:39:25.439626Z","shell.execute_reply":"2025-04-04T10:39:25.785089Z"}},"outputs":[{"name":"stdout","text":"Starting EDA Step 2b: Statistical Analysis of Average Spatial Patterns\nLoading data...\nData loaded.\n\nCalculating average maps...\nAverage maps calculated.\n\n--- Statistics for Average Map: Run 357479 ---\n  Min: 0.0000\n  Max: 948.9780\n  Mean: 181.0826\n  Std Dev: 360.7005\n  Median (50th %): 0.0000\n  75th Percentile: 0.0000\n\n--- Statistics for Average Map: Run 355456 ---\n  Min: 0.0000\n  Max: 1455.4012\n  Mean: 157.1423\n  Std Dev: 363.9954\n  Median (50th %): 0.0000\n  75th Percentile: 0.0000\n\n--- Top 5 Most Active Cells (Highest Average Occupancy) ---\n  Run 357479:\n    #1: iEta=6, iPhi=30 -> Avg Value=948.98\n    #2: iEta=6, iPhi=22 -> Avg Value=948.96\n    #3: iEta=57, iPhi=20 -> Avg Value=948.87\n    #4: iEta=6, iPhi=70 -> Avg Value=948.84\n    #5: iEta=57, iPhi=12 -> Avg Value=948.77\n  Run 355456:\n    #1: iEta=59, iPhi=44 -> Avg Value=1455.40\n    #2: iEta=59, iPhi=4 -> Avg Value=1439.71\n    #3: iEta=59, iPhi=26 -> Avg Value=1436.12\n    #4: iEta=59, iPhi=8 -> Avg Value=1432.68\n    #5: iEta=59, iPhi=12 -> Avg Value=1432.39\n\n--- Analysis of Difference Map (Run357479 - Run355456) ---\n  Difference Map Statistics:\n    Min Diff: -507.9094\n    Max Diff: 554.6357\n    Mean Diff: 23.9403\n    Std Dev Diff: 171.3571\n    Mean Absolute Difference: 70.5804\n\n  Top 5 Largest Positive Differences (Run357479 > Run355456):\n    #1: iEta=50, iPhi=17 -> Diff=554.64 (Vals: 791.64 vs 237.00)\n    #2: iEta=13, iPhi=30 -> Diff=553.40 (Vals: 790.62 vs 237.22)\n    #3: iEta=13, iPhi=35 -> Diff=552.80 (Vals: 810.68 vs 257.87)\n    #4: iEta=50, iPhi=16 -> Diff=552.46 (Vals: 814.17 vs 261.71)\n    #5: iEta=12, iPhi=22 -> Diff=552.18 (Vals: 805.56 vs 253.38)\n\n  Top 5 Largest Negative Differences (Run357479 < Run355456):\n    #1: iEta=59, iPhi=44 -> Diff=-507.91 (Vals: 947.49 vs 1455.40)\n    #2: iEta=59, iPhi=4 -> Diff=-493.25 (Vals: 946.46 vs 1439.71)\n    #3: iEta=59, iPhi=26 -> Diff=-491.96 (Vals: 944.15 vs 1436.12)\n    #4: iEta=59, iPhi=30 -> Diff=-487.60 (Vals: 943.63 vs 1431.22)\n    #5: iEta=59, iPhi=66 -> Diff=-487.04 (Vals: 944.09 vs 1431.13)\n\nEDA Step 2b finished.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import numpy as np\nimport os\n\nfile_path_run357479 = '/kaggle/input/gsoc-test-data/Run357479_Dataset_iodic.npy'\nfile_path_run355456 = '/kaggle/input/gsoc-test-data/Run355456_Dataset_jqkne.npy'\n# ---\n\n# --- Configuration ---\nN_SAMPLES_TO_COMPARE = 3\n# ---\n\nprint(f\"Starting EDA Step 3: Comparing {N_SAMPLES_TO_COMPARE} Individual Slice Examples Statistically\")\n\ntry:\n    # Load data\n    print(\"Loading data...\")\n    # Ensure we load the full arrays, not just memory-mapped objects if that was tried before\n    data_run357479 = np.load(file_path_run357479)\n    data_run355456 = np.load(file_path_run355456)\n    print(\"Data loaded.\")\n\n    # Get number of samples and ensure it's at least N_SAMPLES_TO_COMPARE\n    num_samples = data_run357479.shape[0]\n    if num_samples < N_SAMPLES_TO_COMPARE:\n        print(f\"Warning: Only {num_samples} samples available, reducing comparison count.\")\n        N_SAMPLES_TO_COMPARE = num_samples\n\n    # Generate random indices\n    np.random.seed(42) # for reproducibility\n    random_indices = np.random.choice(num_samples, N_SAMPLES_TO_COMPARE, replace=False)\n    print(f\"Comparing slices at indices: {random_indices}\")\n\n    # Loop through random indices\n    for i, sample_idx in enumerate(random_indices):\n        print(f\"\\n--- Comparing Slice at Index {sample_idx} ---\")\n        \n        # Extract slices\n        slice_357479 = data_run357479[sample_idx] # Shape (64, 72)\n        slice_355456 = data_run355456[sample_idx] # Shape (64, 72)\n\n        # --- Stats for Slice from Run 357479 ---\n        print(\"  Run 357479 Slice Stats:\")\n        if slice_357479.size > 0:\n            print(f\"    Min: {np.min(slice_357479):.2f}\")\n            print(f\"    Max: {np.max(slice_357479):.2f}\")\n            print(f\"    Mean: {np.mean(slice_357479):.2f}\")\n            zeros_perc = (np.count_nonzero(slice_357479==0) / slice_357479.size) * 100\n            print(f\"    Zeros: {zeros_perc:.2f}%\")\n        else: print(\"    Slice empty.\")\n\n        # --- Stats for Slice from Run 355456 ---\n        print(\"  Run 355456 Slice Stats:\")\n        if slice_355456.size > 0:\n            print(f\"    Min: {np.min(slice_355456):.2f}\")\n            print(f\"    Max: {np.max(slice_355456):.2f}\")\n            print(f\"    Mean: {np.mean(slice_355456):.2f}\")\n            zeros_perc = (np.count_nonzero(slice_355456==0) / slice_355456.size) * 100\n            print(f\"    Zeros: {zeros_perc:.2f}%\")\n        else: print(\"    Slice empty.\")\n\n        # --- Stats for Difference (Slice 357479 - Slice 355456) ---\n        if slice_357479.size > 0 and slice_355456.size > 0:\n            print(\"  Difference (Slice 357479 - Slice 355456) Stats:\")\n            diff_slice = slice_357479 - slice_355456\n            print(f\"    Min Diff: {np.min(diff_slice):.2f}\")\n            print(f\"    Max Diff: {np.max(diff_slice):.2f}\")\n            print(f\"    Mean Diff: {np.mean(diff_slice):.2f}\")\n            mean_abs_diff = np.mean(np.abs(diff_slice))\n            print(f\"    Mean Absolute Diff: {mean_abs_diff:.2f}\")\n        else: print(\"  Difference calculation skipped.\")\n\n    print(\"\\nEDA Step 3 finished.\")\n\nexcept FileNotFoundError:\n    print(\"Error: One or both input files not found. Please ensure paths are correct.\")\nexcept Exception as e:\n    print(f\"An error occurred during EDA Step 3: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:39:34.206321Z","iopub.execute_input":"2025-04-04T10:39:34.206643Z","iopub.status.idle":"2025-04-04T10:39:34.471802Z","shell.execute_reply.started":"2025-04-04T10:39:34.206622Z","shell.execute_reply":"2025-04-04T10:39:34.470783Z"}},"outputs":[{"name":"stdout","text":"Starting EDA Step 3: Comparing 3 Individual Slice Examples Statistically\nLoading data...\nData loaded.\nComparing slices at indices: [6252 4684 1731]\n\n--- Comparing Slice at Index 6252 ---\n  Run 357479 Slice Stats:\n    Min: 0.00\n    Max: 940.44\n    Mean: 178.80\n    Zeros: 79.77%\n  Run 355456 Slice Stats:\n    Min: 0.00\n    Max: 1456.67\n    Mean: 155.47\n    Zeros: 79.77%\n  Difference (Slice 357479 - Slice 355456) Stats:\n    Min Diff: -518.84\n    Max Diff: 565.15\n    Mean Diff: 23.33\n    Mean Absolute Diff: 70.17\n\n--- Comparing Slice at Index 4684 ---\n  Run 357479 Slice Stats:\n    Min: 0.00\n    Max: 950.15\n    Mean: 182.54\n    Zeros: 79.77%\n  Run 355456 Slice Stats:\n    Min: 0.00\n    Max: 1447.28\n    Mean: 156.57\n    Zeros: 79.77%\n  Difference (Slice 357479 - Slice 355456) Stats:\n    Min Diff: -501.70\n    Max Diff: 588.11\n    Mean Diff: 25.97\n    Mean Absolute Diff: 71.80\n\n--- Comparing Slice at Index 1731 ---\n  Run 357479 Slice Stats:\n    Min: 0.00\n    Max: 957.50\n    Mean: 184.00\n    Zeros: 79.77%\n  Run 355456 Slice Stats:\n    Min: 0.00\n    Max: 1438.48\n    Mean: 154.88\n    Zeros: 79.77%\n  Difference (Slice 357479 - Slice 355456) Stats:\n    Min Diff: -485.23\n    Max Diff: 607.16\n    Mean Diff: 29.11\n    Mean Absolute Diff: 72.06\n\nEDA Step 3 finished.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nimport os\nimport gc\n\nfile_path_run357479 = '/kaggle/input/gsoc-test-data/Run357479_Dataset_iodic.npy'\nfile_path_run355456 = '/kaggle/input/gsoc-test-data/Run355456_Dataset_jqkne.npy'\n# ---\n\ntry:\n    print(\"Loading data for preprocessing...\")\n    # Label 0: Run 357479\n    # Label 1: Run 355456\n    data_label0 = np.load(file_path_run357479)\n    data_label1 = np.load(file_path_run355456)\n    print(\"Data loaded successfully.\")\n\n    # --- 1. Combine Data ---\n    X = np.concatenate((data_label0, data_label1), axis=0)\n    print(f\"\\n1. Combined data shape (X): {X.shape}\")\n\n    # --- 2. Create Labels ---\n    num_samples_label0 = data_label0.shape[0]\n    num_samples_label1 = data_label1.shape[0]\n    labels0 = np.zeros(num_samples_label0, dtype=np.int32) # Use int32\n    labels1 = np.ones(num_samples_label1, dtype=np.int32)  # Use int32\n    y = np.concatenate((labels0, labels1), axis=0)\n    print(f\"2. Combined labels shape (y): {y.shape}\")\n    print(f\"   Label examples: {y[::5000]}\")\n\n    # --- 3. Reshape for ViT ---\n    X = np.expand_dims(X, axis=-1)\n    print(f\"3. Reshaped data shape (added channel): {X.shape}\")\n\n    # Clean up original large arrays now before splitting\n    del data_label0, data_label1\n    gc.collect()\n\n    # --- 4. Train/Validation/Test Split (BEFORE Normalization) ---\n    print(\"\\n4. Splitting data into Train, Validation, Test sets...\")\n    # Split ratio: 60% train, 20% validation, 20% test\n    X_train_val, X_test, y_train_val, y_test = train_test_split(\n        X, y,\n        test_size=0.20,       # 20% for testing\n        random_state=42,     # For reproducibility\n        stratify=y           # Keep label proportions\n    )\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train_val, y_train_val,\n        test_size=0.25,       # 25% of 80% = 20% of total for validation\n        random_state=42,     # For reproducibility\n        stratify=y_train_val # Stratify again\n    )\n    # Clean up intermediate split\n    del X, y, X_train_val, y_train_val\n    gc.collect()\n\n    print(f\"   Raw Split Shapes:\")\n    print(f\"     Train set:      X={X_train.shape}, y={y_train.shape}\")\n    print(f\"     Validation set: X={X_val.shape}, y={y_val.shape}\")\n    print(f\"     Test set:       X={X_test.shape}, y={y_test.shape}\")\n\n    # --- 5. Normalization (Fit on Train, Apply to All) ---\n    print(\"\\n5. Normalizing data (Min-Max scaling, fitted on Train set)...\")\n    \n    # Calculate min and max *only* on the training data\n    min_val = np.min(X_train)\n    max_val = np.max(X_train)\n    print(f\"   Train set Min value: {min_val}\")\n    print(f\"   Train set Max value: {max_val}\")\n\n    # Apply the scaling to all sets using train min/max\n    if max_val > min_val:\n        X_train = (X_train - min_val) / (max_val - min_val)\n        X_val = (X_val - min_val) / (max_val - min_val)\n        X_test = (X_test - min_val) / (max_val - min_val)\n        \n        # Clip values outside [0, 1] just in case (can happen with test/val data)\n        X_val = np.clip(X_val, 0.0, 1.0)\n        X_test = np.clip(X_test, 0.0, 1.0)\n        \n        print(f\"   Train data normalized. New min={np.min(X_train):.1f}, max={np.max(X_train):.1f}\")\n        print(f\"   Val data normalized.   New min={np.min(X_val):.1f}, max={np.max(X_val):.1f}\")\n        print(f\"   Test data normalized.  New min={np.min(X_test):.1f}, max={np.max(X_test):.1f}\")\n    else:\n        print(\"   Skipping normalization as training data range is zero.\")\n\n    # --- 6. Data Type Conversion ---\n    print(\"\\n6. Converting data arrays to float32...\")\n    X_train = X_train.astype(np.float32)\n    X_val = X_val.astype(np.float32)\n    X_test = X_test.astype(np.float32)\n    # Labels should already be int32 from step 2\n\n    print(f\"   Final dtypes: X_train={X_train.dtype}, y_train={y_train.dtype}\")\n    print(f\"   Final shapes:\")\n    print(f\"     Train set:      X={X_train.shape}, y={y_train.shape}\")\n    print(f\"     Validation set: X={X_val.shape}, y={y_val.shape}\")\n    print(f\"     Test set:       X={X_test.shape}, y={y_test.shape}\")\n\n    print(\"\\nPreprocessing finished. Data splits (X_train, y_train, etc.) are ready.\")\n    \n\nexcept FileNotFoundError:\n    print(\"Error: One or both input files not found. Please ensure paths are correct.\")\nexcept Exception as e:\n    print(f\"An error occurred during preprocessing: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:39:43.078704Z","iopub.execute_input":"2025-04-04T10:39:43.079027Z","iopub.status.idle":"2025-04-04T10:39:45.535079Z","shell.execute_reply.started":"2025-04-04T10:39:43.079003Z","shell.execute_reply":"2025-04-04T10:39:45.534049Z"}},"outputs":[{"name":"stdout","text":"Loading data for preprocessing...\nData loaded successfully.\n\n1. Combined data shape (X): (20000, 64, 72)\n2. Combined labels shape (y): (20000,)\n   Label examples: [0 0 1 1]\n3. Reshaped data shape (added channel): (20000, 64, 72, 1)\n\n4. Splitting data into Train, Validation, Test sets...\n   Raw Split Shapes:\n     Train set:      X=(12000, 64, 72, 1), y=(12000,)\n     Validation set: X=(4000, 64, 72, 1), y=(4000,)\n     Test set:       X=(4000, 64, 72, 1), y=(4000,)\n\n5. Normalizing data (Min-Max scaling, fitted on Train set)...\n   Train set Min value: 0.0\n   Train set Max value: 1564.944737802157\n   Train data normalized. New min=0.0, max=1.0\n   Val data normalized.   New min=0.0, max=1.0\n   Test data normalized.  New min=0.0, max=1.0\n\n6. Converting data arrays to float32...\n   Final dtypes: X_train=float32, y_train=int32\n   Final shapes:\n     Train set:      X=(12000, 64, 72, 1), y=(12000,)\n     Validation set: X=(4000, 64, 72, 1), y=(4000,)\n     Test set:       X=(4000, 64, 72, 1), y=(4000,)\n\nPreprocessing finished. Data splits (X_train, y_train, etc.) are ready.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np # Keep numpy imported\n\n# --- Define Hyperparameters ---\nIMAGE_HEIGHT = 64\nIMAGE_WIDTH = 72\nNUM_CHANNELS = 1\nIMAGE_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)\n\nPATCH_SIZE = (8, 9) # Results in 8x8=64 patches\nNUM_PATCHES = (IMAGE_HEIGHT // PATCH_SIZE[0]) * (IMAGE_WIDTH // PATCH_SIZE[1])\nPROJECTION_DIM = 128 # Embedding dimension\nNUM_HEADS = 4 # Number of attention heads\nTRANSFORMER_LAYERS = 4 # Number of transformer blocks\nMLP_HEAD_UNITS = [64] # Classification head dense layers\nNUM_CLASSES = 1 # Binary classification output\n\nprint(\"--- Hyperparameters ---\")\nprint(f\"Input Image Shape: {IMAGE_SHAPE}\")\nprint(f\"Patch Size: {PATCH_SIZE}\")\nprint(f\"Number of Patches: {NUM_PATCHES}\")\nprint(f\"Projection Dim (Embedding Size): {PROJECTION_DIM}\")\nprint(f\"Number of Attention Heads: {NUM_HEADS}\")\nprint(f\"Number of Transformer Layers: {TRANSFORMER_LAYERS}\")\nprint(f\"MLP Head Units: {MLP_HEAD_UNITS}\")\nprint(f\"Number of Output Classes: {NUM_CLASSES}\")\nprint(\"-----------------------\")\n\n# --- Layer Definitions ---\n\n# Patch Creation Layer\nclass Patches(layers.Layer):\n    def __init__(self, patch_size, **kwargs):\n        super().__init__(**kwargs)\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size[0], self.patch_size[1], 1],\n            strides=[1, self.patch_size[0], self.patch_size[1], 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches\n        \n    # Need get_config for saving/loading models with custom layers\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"patch_size\": self.patch_size})\n        return config\n\n# Patch Encoding Layer (Learnable projection + Position embedding)\nclass PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded\n        \n    # Need get_config for saving/loading models with custom layers\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection.units})\n        return config\n\n# --- ViT Builder Function ---\ndef build_vit_classifier(input_shape, patch_size, num_patches, projection_dim,\n                         num_heads, transformer_layers, mlp_head_units, num_classes):\n    inputs = layers.Input(shape=input_shape)\n\n    # Create patches.\n    patches = Patches(patch_size)(inputs)\n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        # Ensure key_dim is compatible with projection_dim and num_heads\n        if projection_dim % num_heads != 0:\n             raise ValueError(f\"projection_dim ({projection_dim}) must be divisible by num_heads ({num_heads})\")\n        key_dim = projection_dim // num_heads\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=key_dim, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP inside transformer block\n        # Standard practice: expand to 2x or 4x projection_dim then contract back\n        mlp_intermediate_dim = projection_dim * 2 \n        x3 = layers.Dense(units=mlp_intermediate_dim, activation=tf.nn.gelu)(x3)\n        x3 = layers.Dropout(0.1)(x3)\n        x3 = layers.Dense(units=projection_dim)(x3) # No activation on final MLP layer in block? Check common practice. Usually yes. Let's add GELU.\n        x3 = layers.Activation(tf.nn.gelu)(x3)\n        x3 = layers.Dropout(0.1)(x3)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Final part: Classification Head\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    # Global average pooling across patches\n    representation = layers.GlobalAveragePooling1D()(representation)\n    representation = layers.Dropout(0.5)(representation) # Increased dropout before final head\n\n    # MLP Head layers\n    features = representation\n    for units in mlp_head_units:\n        features = layers.Dense(units=units, activation=tf.nn.relu)(features) # Using ReLU here\n        features = layers.Dropout(0.3)(features) # Dropout within head\n\n    # Final classification layer\n    logits = layers.Dense(num_classes, activation=\"sigmoid\")(features) # Sigmoid for binary output\n\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model\n\n# --- Build the Model ---\nprint(\"\\nBuilding the ViT model...\")\n# Ensure variables defined in the preprocessing step are available if needed for input_shape\n# Or redefine input_shape directly as we know it\ninput_shape_for_model = IMAGE_SHAPE\n\n# Handle potential errors during build\ntry:\n    vit_classifier = build_vit_classifier(\n        input_shape=input_shape_for_model,\n        patch_size=PATCH_SIZE,\n        num_patches=NUM_PATCHES,\n        projection_dim=PROJECTION_DIM,\n        num_heads=NUM_HEADS,\n        transformer_layers=TRANSFORMER_LAYERS,\n        mlp_head_units=MLP_HEAD_UNITS,\n        num_classes=NUM_CLASSES\n    )\n    \n    # Print the model summary\n    print(\"\\nModel Summary:\")\n    vit_classifier.summary()\n    print(\"\\nViT model built successfully.\")\n\nexcept ValueError as ve:\n    print(f\"\\nError building model: {ve}\")\n    print(\"Please check hyperparameters (e.g., projection_dim divisibility by num_heads).\")\nexcept Exception as e:\n    print(f\"\\nAn unexpected error occurred during model building: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:22:36.077245Z","iopub.execute_input":"2025-04-04T10:22:36.077779Z","iopub.status.idle":"2025-04-04T10:22:55.038757Z","shell.execute_reply.started":"2025-04-04T10:22:36.077752Z","shell.execute_reply":"2025-04-04T10:22:55.037609Z"}},"outputs":[{"name":"stdout","text":"--- Hyperparameters ---\nInput Image Shape: (64, 72, 1)\nPatch Size: (8, 9)\nNumber of Patches: 64\nProjection Dim (Embedding Size): 128\nNumber of Attention Heads: 4\nNumber of Transformer Layers: 4\nMLP Head Units: [64]\nNumber of Output Classes: 1\n-----------------------\n\nBuilding the ViT model...\n\nModel Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ patches (\u001b[38;5;33mPatches\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ patch_encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m17,536\u001b[0m │ patches[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│ (\u001b[38;5;33mPatchEncoder\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ patch_encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n│                           │                        │                │ patch_encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m32,896\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_2… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_3… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m32,896\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_3 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_2    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_4… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_4 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_5… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m32,896\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_5 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_6… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_6 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m32,896\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_3 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_7 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                           │                        │                │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_8… │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ patches (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Patches</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ patch_encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17,536</span> │ patches[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEncoder</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ patch_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                           │                        │                │ patch_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_2… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_3… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_2    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_4… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_5… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_6… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                           │                        │                │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_8… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m556,033\u001b[0m (2.12 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">556,033</span> (2.12 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m556,033\u001b[0m (2.12 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">556,033</span> (2.12 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\nViT model built successfully.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nimport time # To time training\nimport gc\n\nprint(tf.__version__) # Check TensorFlow version\n\n# --- Configuration ---\nEPOCHS = 50 \nBATCH_SIZE_PER_REPLICA = 32 # Adjust based on GPU memory if needed\nLEARNING_RATE = 3e-4 # Starting learning rate for AdamW\n\n# --- 1. Distribution Strategy ---\ntry:\n    strategy = tf.distribute.MirroredStrategy()\n    NUM_GPUS = strategy.num_replicas_in_sync\n    print(f'Number of devices: {NUM_GPUS}')\n    if NUM_GPUS < 2:\n        print(\"Warning: MirroredStrategy found fewer than 2 devices. Check GPU detection.\")\nexcept Exception as e:\n    print(\"Error setting up MirroredStrategy, falling back to default strategy.\")\n    print(f\"Error: {e}\")\n    strategy = tf.distribute.get_strategy() # Default strategy\n    NUM_GPUS = 1\n\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * NUM_GPUS\nprint(f'Global Batch Size: {GLOBAL_BATCH_SIZE}')\n\n# --- 2. Data Pipeline ---\n# Buffer size for shuffling (should be larger than batch size, ideally size of dataset for perfect shuffle)\nBUFFER_SIZE = len(X_train) \n\nprint(\"\\nCreating tf.data Datasets...\")\n# Training Dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_dataset = train_dataset.cache() # Cache data in memory after first epoch\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(tf.data.AUTOTUNE) # Prefetch batches\n\n# Validation Dataset\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\nval_dataset = val_dataset.cache() # Cache validation data\nval_dataset = val_dataset.batch(GLOBAL_BATCH_SIZE) # Use global batch size for validation too\nval_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n\nprint(\"Training and Validation Datasets created.\")\nprint(f\"  Training dataset element spec: {train_dataset.element_spec}\")\n\n\n# --- 3. Build & Compile Model within Strategy Scope ---\nprint(\"\\nBuilding and Compiling Model under Strategy Scope...\")\nwith strategy.scope():\n    \n    try:\n         # Use previously defined hyperparameters\n         vit_classifier_dist = build_vit_classifier(\n             input_shape=IMAGE_SHAPE, patch_size=PATCH_SIZE, num_patches=NUM_PATCHES,\n             projection_dim=PROJECTION_DIM, num_heads=NUM_HEADS, transformer_layers=TRANSFORMER_LAYERS,\n             mlp_head_units=MLP_HEAD_UNITS, num_classes=NUM_CLASSES\n         )\n         \n         # Compile the model\n         optimizer = tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-5)\n         vit_classifier_dist.compile(\n             optimizer=optimizer,\n             loss=tf.keras.losses.BinaryCrossentropy(),\n             metrics=[\n                 tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n                 tf.keras.metrics.AUC(name='auc'),\n                 tf.keras.metrics.AUC(name='prc', curve='PR') # Area under Precision-Recall curve\n             ]\n         )\n         print(\"Model built and compiled successfully.\")\n         vit_classifier_dist.summary() # Optionally print summary again\n         \n    except NameError as ne:\n         print(f\"Error: Ensure 'build_vit_classifier' function and layers are defined before this block. {ne}\")\n         # Handle error gracefully, maybe exit or raise\n         raise ne\n    except Exception as e:\n         print(f\"An unexpected error occurred during model build/compile: {e}\")\n         raise e\n\n\n# --- 4. Callbacks ---\nprint(\"\\nSetting up Callbacks...\")\n# Path where checkpoint will be saved\ncheckpoint_filepath = 'vit_model_checkpoint.keras'\n\n# Save the best model based on validation AUC\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False, # Save the whole model\n    monitor='val_auc',    # Monitor validation AUC\n    mode='max',           # Maximize AUC\n    save_best_only=True)  # Only save if improved\n\n# Stop training early if validation AUC doesn't improve for 'patience' epochs\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc',\n    patience=10,          # Number of epochs with no improvement after which training will be stopped.\n    mode='max',\n    restore_best_weights=True, # Restore model weights from the epoch with the best value of the monitored quantity.\n    verbose=1\n)\n\n\ncallbacks_list = [\n    model_checkpoint_callback,\n    early_stopping_callback,\n]\nprint(\"Callbacks defined: ModelCheckpoint (on val_auc), EarlyStopping (on val_auc)\")\n\n# Clean up potentially large variables before training\n# del X_train, y_train, X_val, y_val # Keep if needed later, but tf.data should have them\ngc.collect()\n\n# --- 5. Training ---\nprint(\"\\n--- Starting Model Training ---\")\nstart_time = time.time()\n\nhistory = vit_classifier_dist.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=val_dataset,\n    callbacks=callbacks_list\n)\n\nend_time = time.time()\nprint(f\"--- Training Finished ---\")\nprint(f\"Total Training Time: {(end_time - start_time):.2f} seconds\")\n\n\n# Find the epoch with the best validation AUC\nbest_epoch = np.argmax(history.history['val_auc'])\nbest_val_auc = np.max(history.history['val_auc'])\nprint(f\"\\nBest Validation AUC: {best_val_auc:.4f} at Epoch: {best_epoch + 1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:25:37.220217Z","iopub.execute_input":"2025-04-04T10:25:37.221034Z","iopub.status.idle":"2025-04-04T10:28:11.667251Z","shell.execute_reply.started":"2025-04-04T10:25:37.221003Z","shell.execute_reply":"2025-04-04T10:28:11.666315Z"}},"outputs":[{"name":"stdout","text":"2.17.1\nNumber of devices: 2\nGlobal Batch Size: 64\n\nCreating tf.data Datasets...\nTraining and Validation Datasets created.\n  Training dataset element spec: (TensorSpec(shape=(None, 64, 72, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n\nBuilding and Compiling Model under Strategy Scope...\nModel built and compiled successfully.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ patches_1 (\u001b[38;5;33mPatches\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ patch_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m17,536\u001b[0m │ patches_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n│ (\u001b[38;5;33mPatchEncoder\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_9     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ patch_encoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_4    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_9… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_9… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_8 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ patch_encoder_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_10    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_12 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_13 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m32,896\u001b[0m │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_4 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_9 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                           │                        │                │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_11    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_5    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_1… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_10 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_14 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_15 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m32,896\u001b[0m │ dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_5 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_11 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                           │                        │                │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_6    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_1… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_12 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_16 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_17 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m32,896\u001b[0m │ dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_6 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_13 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                           │                        │                │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_15    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_7    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_1… │\n│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_14 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n│                           │                        │                │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_16    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_18 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m33,024\u001b[0m │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_19 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m32,896\u001b[0m │ dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_7 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_15 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                           │                        │                │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_17    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_1… │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_20 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_21 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ patches_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Patches</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ patch_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17,536</span> │ patches_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEncoder</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_9     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ patch_encoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_4    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_9… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_9… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ patch_encoder_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_10    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                           │                        │                │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_11    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_5    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                           │                        │                │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_6    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                           │                        │                │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_15    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multi_head_attention_7    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n│                           │                        │                │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_16    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ layer_normalization_1… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                           │                        │                │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ layer_normalization_17    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m556,033\u001b[0m (2.12 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">556,033</span> (2.12 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m556,033\u001b[0m (2.12 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">556,033</span> (2.12 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\nSetting up Callbacks...\nCallbacks defined: ModelCheckpoint (on val_auc), EarlyStopping (on val_auc)\n\n--- Starting Model Training ---\nEpoch 1/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 62ms/step - accuracy: 0.8755 - auc: 0.9352 - loss: 0.2425 - prc: 0.9350 - val_accuracy: 0.9990 - val_auc: 0.9993 - val_loss: 0.0086 - val_prc: 0.9987\nEpoch 2/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 50ms/step - accuracy: 0.9991 - auc: 0.9993 - loss: 0.0089 - prc: 0.9986 - val_accuracy: 0.9980 - val_auc: 0.9980 - val_loss: 0.0144 - val_prc: 0.9960\nEpoch 3/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 50ms/step - accuracy: 0.9994 - auc: 0.9994 - loss: 0.0057 - prc: 0.9988 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 4.1727e-05 - val_prc: 1.0000\nEpoch 4/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.5825e-04 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 2.0585e-05 - val_prc: 1.0000\nEpoch 5/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.3377e-04 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 1.0563e-05 - val_prc: 1.0000\nEpoch 6/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 2.0071e-04 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 7.1869e-06 - val_prc: 1.0000\nEpoch 7/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 2.1101e-04 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 4.6821e-06 - val_prc: 1.0000\nEpoch 8/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 1.1280e-04 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 3.4669e-06 - val_prc: 1.0000\nEpoch 9/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 1.0052e-04 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 2.4774e-06 - val_prc: 1.0000\nEpoch 10/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.2478e-05 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 1.8161e-06 - val_prc: 1.0000\nEpoch 11/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.3443e-05 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 1.4369e-06 - val_prc: 1.0000\nEpoch 12/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 7.4380e-05 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 1.1053e-06 - val_prc: 1.0000\nEpoch 13/50\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.1616e-05 - prc: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 8.2664e-07 - val_prc: 1.0000\nEpoch 13: early stopping\nRestoring model weights from the end of the best epoch: 3.\n--- Training Finished ---\nTotal Training Time: 152.77 seconds\n\nBest Validation AUC: 1.0000 at Epoch: 3\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport os\nimport gc\nfrom sklearn.metrics import roc_curve, auc # For ROC curve later\nimport matplotlib.pyplot as plt # For ROC curve later\n\n\nclass Patches(layers.Layer):\n    def __init__(self, patch_size, **kwargs):\n        super().__init__(**kwargs)\n        self.patch_size = tuple(patch_size)\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size[0], self.patch_size[1], 1],\n            strides=[1, self.patch_size[0], self.patch_size[1], 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        num_patches_h = tf.shape(images)[1] // self.patch_size[0]\n        num_patches_w = tf.shape(images)[2] // self.patch_size[1]\n        num_patches_total = num_patches_h * num_patches_w\n        patches = tf.reshape(patches, [batch_size, num_patches_total, patch_dims])\n        return patches\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"patch_size\": self.patch_size})\n        return config\n\nclass PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.num_patches = num_patches\n        self.projection_dim = projection_dim\n        # Define layers within __init__ for proper tracking\n        # Use names that likely match how they were saved\n        self.projection = layers.Dense(units=projection_dim, name='projection')\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim, name='position_embedding' # Check name consistency\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection_dim})\n        return config\n\n\n# --- Evaluation on Test Set ---\nprint(\"\\n--- Evaluating Best Model on Test Set ---\")\n\n\ntry:\n    if 'X_test' not in locals() or 'y_test' not in locals():\n         saved_splits_path = 'processed_splits.npz'\n         if os.path.exists(saved_splits_path):\n             print(f\"Test data (X_test, y_test) not found in memory. Loading from {saved_splits_path}...\")\n             with np.load(saved_splits_path) as data:\n                 X_test = data['X_test']\n                 y_test = data['y_test']\n             print(\"Test data loaded.\")\n         else:\n             raise NameError(\"Test data (X_test, y_test) not found. Please ensure preprocessing output is loaded or saved file exists.\")\n    else:\n        print(\"Using existing X_test, y_test from memory.\")\n\n    # Define path to the saved model checkpoint\n    checkpoint_filepath = 'vit_model_checkpoint.keras'\n\n    if os.path.exists(checkpoint_filepath):\n        print(f\"Loading best model from: {checkpoint_filepath}\")\n\n       \n        custom_objects = {\"Patches\": Patches, \"PatchEncoder\": PatchEncoder}\n        loaded_model = tf.keras.models.load_model(\n            checkpoint_filepath,\n            custom_objects=custom_objects\n        )\n\n        print(\"Model loaded successfully.\")\n\n        # Re-compile the loaded model\n        loaded_model.compile(\n             loss=tf.keras.losses.BinaryCrossentropy(),\n             metrics=[\n                 tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n                 tf.keras.metrics.AUC(name='auc'),\n                 tf.keras.metrics.AUC(name='prc', curve='PR')\n             ]\n        )\n        print(\"Model re-compiled for evaluation.\")\n\n        # Create tf.data.Dataset for test set\n        try:\n            if 'GLOBAL_BATCH_SIZE' not in locals(): GLOBAL_BATCH_SIZE = 64\n        except NameError: GLOBAL_BATCH_SIZE = 64\n\n        test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n        test_dataset = test_dataset.batch(GLOBAL_BATCH_SIZE)\n        test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n\n        print(f\"\\nEvaluating on Test Set (using Batch Size: {GLOBAL_BATCH_SIZE})...\")\n        results = loaded_model.evaluate(test_dataset, verbose=1)\n\n        print(\"\\n--- Test Set Results ---\")\n        if results and len(results) > 3:\n            test_loss = results[0]\n            test_accuracy = results[1]\n            test_auc = results[2]\n            test_prc = results[3]\n            print(f\"Test Loss:     {test_loss:.4f}\")\n            print(f\"Test Accuracy: {test_accuracy:.4f}\")\n            print(f\"Test AUC:      {test_auc:.4f}\")\n            print(f\"Test PRC (AUC):{test_prc:.4f}\")\n        else:\n            print(f\"Evaluation returned unexpected result format: {results}\")\n\n    else:\n        print(f\"Error: Model checkpoint file not found at {checkpoint_filepath}\")\n        print(\"Please ensure training ran and saved the checkpoint.\")\n\nexcept NameError as ne:\n     print(f\"Error accessing data or variables: {ne}\")\nexcept FileNotFoundError as fnfe:\n     print(f\"Error: A required file was not found: {fnfe}\")\nexcept Exception as e:\n    # Print detailed traceback for unexpected errors\n    import traceback\n    print(f\"An error occurred during test set evaluation:\")\n    traceback.print_exc()\n\n\ntry:\n    if 'loaded_model' in locals() and 'X_test' in locals() and 'y_test' in locals():\n        print(\"\\nGenerating ROC curve for the test set...\")\n        y_pred_proba = loaded_model.predict(X_test)\n\n        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n        roc_auc = auc(fpr, tpr) # Calculate AUC from fpr, tpr\n\n        plt.figure(figsize=(8, 6))\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ViT ROC curve (AUC = {roc_auc:.4f})')\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance (AUC = 0.5)')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate (FPR)')\n        plt.ylabel('True Positive Rate (TPR)')\n        plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')\n        plt.legend(loc=\"lower right\")\n        plt.grid(True)\n        plt.savefig('roc_curve_test_set.png')\n        print(\"ROC curve saved to roc_curve_test_set.png\")\n        plt.close()\n    else:\n        print(\"\\nSkipping ROC curve generation as model or test data is not available.\")\nexcept Exception as e:\n    print(f\"\\nAn error occurred during ROC curve generation: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:33:38.819288Z","iopub.execute_input":"2025-04-04T10:33:38.819657Z","iopub.status.idle":"2025-04-04T10:33:51.056214Z","shell.execute_reply.started":"2025-04-04T10:33:38.819633Z","shell.execute_reply":"2025-04-04T10:33:51.055133Z"}},"outputs":[{"name":"stdout","text":"\n--- Evaluating Best Model on Test Set ---\nUsing existing X_test, y_test from memory.\nLoading best model from: vit_model_checkpoint.keras\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'patch_encoder_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully.\nModel re-compiled for evaluation.\n\nEvaluating on Test Set (using Batch Size: 64)...\n\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.8766e-05 - prc: 1.0000\n\n--- Test Set Results ---\nTest Loss:     0.0000\nTest Accuracy: 1.0000\nTest AUC:      1.0000\nTest PRC (AUC):1.0000\n\nGenerating ROC curve for the test set...\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\nROC curve saved to roc_curve_test_set.png\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport os\nimport gc\n\n\nclass Patches(layers.Layer):\n    def __init__(self, patch_size, **kwargs):\n        super().__init__(**kwargs)\n        self.patch_size = tuple(patch_size)\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size[0], self.patch_size[1], 1],\n            strides=[1, self.patch_size[0], self.patch_size[1], 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        num_patches_h = tf.shape(images)[1] // self.patch_size[0]\n        num_patches_w = tf.shape(images)[2] // self.patch_size[1]\n        num_patches_total = num_patches_h * num_patches_w\n        patches = tf.reshape(patches, [batch_size, num_patches_total, patch_dims])\n        return patches\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"patch_size\": self.patch_size})\n        return config\n\nclass PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.num_patches = num_patches\n        self.projection_dim = projection_dim\n        # Define layers within __init__ for proper tracking\n        self.projection = layers.Dense(units=projection_dim, name='projection')\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim, name='position_embedding' # Check name consistency\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection_dim})\n        return config\n\n\n# --- Show Predictions vs Actuals ---\nprint(\"\\n--- Showing Predictions vs Actuals for Test Set Samples ---\")\n\nN_SAMPLES_TO_SHOW = 15 # Number of samples to display\n\ntry:\n    # Check if necessary variables exist, reload if needed\n    if 'loaded_model' not in locals():\n        print(\"Model 'loaded_model' not found. Attempting to load...\")\n        checkpoint_filepath = 'vit_model_checkpoint.keras'\n        if os.path.exists(checkpoint_filepath):\n             # *** Using custom_objects dictionary for loading - NO DECORATORS NEEDED ***\n             custom_objects_dict = {\"Patches\": Patches, \"PatchEncoder\": PatchEncoder}\n             loaded_model = tf.keras.models.load_model(\n                 checkpoint_filepath,\n                 custom_objects=custom_objects_dict # Pass the dictionary here\n             )\n             print(\"Model loaded successfully using custom_objects.\")\n        else:\n             raise NameError(\"Model checkpoint not found and 'loaded_model' not in memory.\")\n\n    if 'X_test' not in locals() or 'y_test' not in locals():\n        print(\"Test data (X_test, y_test) not found. Attempting to load...\")\n        saved_splits_path = 'processed_splits.npz' # Adjust if needed\n        if os.path.exists(saved_splits_path):\n             with np.load(saved_splits_path) as data:\n                 X_test = data['X_test']\n                 y_test = data['y_test']\n             print(\"Test data loaded.\")\n        else:\n             raise NameError(\"Test data not available.\")\n\n    actual_samples_in_test = len(X_test)\n    if actual_samples_in_test < N_SAMPLES_TO_SHOW:\n        print(f\"Warning: Only {actual_samples_in_test} samples in test set, showing all.\")\n        N_SAMPLES_TO_SHOW = actual_samples_in_test\n\n    if N_SAMPLES_TO_SHOW == 0:\n         print(\"No samples available in the test set to show.\")\n    else:\n        X_test_subset = X_test[:N_SAMPLES_TO_SHOW]\n        y_test_subset = y_test[:N_SAMPLES_TO_SHOW]\n\n        print(f\"\\nGetting predictions for the first {N_SAMPLES_TO_SHOW} test samples...\")\n        try:\n             if 'GLOBAL_BATCH_SIZE' not in locals(): GLOBAL_BATCH_SIZE = 64\n        except NameError: GLOBAL_BATCH_SIZE = 64\n\n        y_pred_proba_subset = loaded_model.predict(X_test_subset, batch_size=GLOBAL_BATCH_SIZE)\n        y_pred_labels_subset = (y_pred_proba_subset > 0.5).astype(int).flatten()\n\n        # Print results\n        print(\"\\n-----------------------------------------------------\")\n        print(\"| Idx | True Label | Predicted Probability | Pred Label |\")\n        print(\"-----------------------------------------------------\")\n        for i in range(N_SAMPLES_TO_SHOW):\n            true_label = y_test_subset[i]\n            pred_proba = y_pred_proba_subset[i][0] if y_pred_proba_subset.ndim > 1 else y_pred_proba_subset[i]\n            pred_label = y_pred_labels_subset[i]\n            # Format probability using scientific notation for very small/large values\n            print(f\"| {i:<3} | {true_label:<10} | {pred_proba:<21.5e} | {pred_label:<10} |\")\n        print(\"-----------------------------------------------------\")\n\nexcept NameError as ne:\n     print(f\"Error: Necessary variable not found. {ne}\")\nexcept FileNotFoundError as fnfe:\n     print(f\"Error: A required file was not found. {fnfe}\")\nexcept Exception as e:\n    import traceback\n    print(f\"An error occurred:\")\n    traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:43:47.561446Z","iopub.execute_input":"2025-04-04T10:43:47.561910Z","iopub.status.idle":"2025-04-04T10:43:50.437927Z","shell.execute_reply.started":"2025-04-04T10:43:47.561869Z","shell.execute_reply":"2025-04-04T10:43:50.437116Z"}},"outputs":[{"name":"stdout","text":"\n--- Showing Predictions vs Actuals for Test Set Samples ---\n\nGetting predictions for the first 15 test samples...\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n\n-----------------------------------------------------\n| Idx | True Label | Predicted Probability | Pred Label |\n-----------------------------------------------------\n| 0   | 1          | 9.99972e-01           | 1          |\n| 1   | 0          | 4.82919e-05           | 0          |\n| 2   | 0          | 4.78464e-05           | 0          |\n| 3   | 1          | 9.99972e-01           | 1          |\n| 4   | 0          | 4.78514e-05           | 0          |\n| 5   | 0          | 4.77857e-05           | 0          |\n| 6   | 1          | 9.99972e-01           | 1          |\n| 7   | 0          | 4.83697e-05           | 0          |\n| 8   | 1          | 9.99972e-01           | 1          |\n| 9   | 1          | 9.99972e-01           | 1          |\n| 10  | 1          | 9.99972e-01           | 1          |\n| 11  | 1          | 9.99972e-01           | 1          |\n| 12  | 1          | 9.99972e-01           | 1          |\n| 13  | 1          | 9.99972e-01           | 1          |\n| 14  | 1          | 9.99970e-01           | 1          |\n-----------------------------------------------------\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"#  Reflections and Potential Extensions on Test 1 Results\n\nI’ve successfully completed the primary objective for **Test 1**, developing a **Vision Transformer (ViT)** that achieved **perfect classification** (Accuracy = 1.0, AUC = 1.0) on the test set using the provided **synthetic HCAL DigiOccupancy data**. The model was trained efficiently using a distributed strategy across multiple GPUs.\n\n---\n\n##  Understanding the Perfect Score\n\nWhile achieving a perfect score is certainly encouraging, it's important to interpret it in context:\n\n- The **synthetic nature** of the data might make the classes **perfectly separable**.\n- The model achieved perfect validation metrics **within just 3 epochs**, indicating strong signal separation.\n- In a real-world scenario, such results might warrant concerns around **data leakage** or overly simple patterns, but here it likely reflects the characteristics of the generated datasets.\n\n---\n\n##  Potential Next Steps to Stand Out\n\nTo go beyond the core objectives and demonstrate a **deeper understanding and initiative**, I could pursue the following extensions:\n\n---\n\n### 1.  Model Interpretability (Attention Maps)\n\n- Visualize **attention maps** from the ViT to understand which regions (iEta/iPhi) contribute most to classification.\n- This would involve:\n  - Extracting **attention weights** from the transformer blocks.\n  - Overlaying attention maps on the input images or visualizing them separately per layer/head.\n- This step could **highlight spatial features** that the model focuses on, linking them to observed differences (e.g., hotspot locations in EDA).\n\n---\n\n### 2.  Baseline Model Comparison\n\nTo understand the **added value of ViT**, implement and evaluate **simpler models** as baselines:\n\n- **Basic CNN** with a few convolutional layers.\n- **Logistic Regression** on:\n  - Flattened images, or  \n  - Aggregated statistics (e.g., max/mean occupancy per slice).\n\n> This would help determine if the **transformer’s complexity** was necessary or if simpler models could achieve similar performance.\n\n---\n\n### 3.  Enhanced Report Discussion\n\nInclude a dedicated section in the final report:\n\n- Discuss the **implications of a perfect score**.\n- Acknowledge that performance on **synthetic data** might not generalize to real-world detector data.\n- Emphasize the **limitations** and **generalization challenges** expected with real data.\n\n---\n\n### 4.  Further Exploration (Optional)\n\n#### a. Mixture-of-Experts (MoE) ViT\n\n- Explore the **MoE-ViT** architecture mentioned in the prompt.\n- Compare:\n  - **Parameter efficiency**\n  - **Training speed**\n  - **Generalization capability** vs. standard ViT\n\n#### b. Ablation / Sensitivity Analysis\n\n- Train a **smaller ViT**:\n  - Fewer layers\n  - Fewer heads\n- Assess how much capacity is **really needed** to solve this classification task.\n\n---\n\n## Summary\n\nBy integrating steps like:\n\n- **Attention map visualization**\n- **Baseline model benchmarking**\n- **Critical analysis of synthetic data limitations**\n\nI can deliver a **well-rounded** and **insightful** submission that goes beyond just meeting the minimum criteria and reflects a deeper engagement with the problem.\n\n---\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}